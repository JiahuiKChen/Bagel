{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44b56b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2025 Bytedance Ltd. and/or its affiliates.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from typing import (\n",
    "    Any,\n",
    "    AsyncIterable,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    Generator,\n",
    "    List,\n",
    "    NamedTuple,\n",
    "    Optional,\n",
    "    Tuple,\n",
    "    Union,\n",
    ")\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from accelerate import infer_auto_device_map, load_checkpoint_and_dispatch, init_empty_weights\n",
    "\n",
    "from data.transforms import ImageTransform\n",
    "from data.data_utils import pil_img2rgb, add_special_tokens\n",
    "from modeling.bagel import (\n",
    "    BagelConfig, Bagel, Qwen2Config, Qwen2ForCausalLM, SiglipVisionConfig, SiglipVisionModel\n",
    ")\n",
    "from modeling.qwen2 import Qwen2Tokenizer\n",
    "from modeling.bagel.qwen2_navit import NaiveCache\n",
    "from modeling.autoencoder import load_ae\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "save_dir = \"/checkpoint/dream_v2/transfusion/BAGEL-7B-MoT\"\n",
    "# repo_id = \"ByteDance-Seed/BAGEL-7B-MoT\"\n",
    "# cache_dir = save_dir + \"/cache\"\n",
    "\n",
    "# snapshot_download(cache_dir=cache_dir,\n",
    "#   local_dir=save_dir,\n",
    "#   repo_id=repo_id,\n",
    "#   local_dir_use_symlinks=False,\n",
    "#   resume_download=True,\n",
    "#   allow_patterns=[\"*.json\", \"*.safetensors\", \"*.bin\", \"*.py\", \"*.md\", \"*.txt\"],\n",
    "# )\n",
    "model_path = save_dir  # Download from https://huggingface.co/ByteDance-Seed/BAGEL-7B-MoT\n",
    "\n",
    "# LLM config preparing\n",
    "llm_config = Qwen2Config.from_json_file(os.path.join(model_path, \"llm_config.json\"))\n",
    "llm_config.qk_norm = True\n",
    "llm_config.tie_word_embeddings = False\n",
    "llm_config.layer_module = \"Qwen2MoTDecoderLayer\"\n",
    "\n",
    "# ViT config preparing\n",
    "vit_config = SiglipVisionConfig.from_json_file(os.path.join(model_path, \"vit_config.json\"))\n",
    "vit_config.rope = False\n",
    "vit_config.num_hidden_layers = vit_config.num_hidden_layers - 1\n",
    "\n",
    "# VAE loading\n",
    "vae_model, vae_config = load_ae(local_path=os.path.join(model_path, \"ae.safetensors\"))\n",
    "\n",
    "# Bagel config preparing\n",
    "config = BagelConfig(\n",
    "    visual_gen=True,\n",
    "    visual_und=True,\n",
    "    llm_config=llm_config, \n",
    "    vit_config=vit_config,\n",
    "    vae_config=vae_config,\n",
    "    vit_max_num_patch_per_side=70,\n",
    "    connector_act='gelu_pytorch_tanh',\n",
    "    latent_patch_size=2,\n",
    "    max_latent_size=64,\n",
    ")\n",
    "\n",
    "with init_empty_weights():\n",
    "    language_model = Qwen2ForCausalLM(llm_config)\n",
    "    vit_model      = SiglipVisionModel(vit_config)\n",
    "    model          = Bagel(language_model, vit_model, config)\n",
    "    model.vit_model.vision_model.embeddings.convert_conv2d_to_linear(vit_config, meta=True)\n",
    "\n",
    "# Tokenizer Preparing\n",
    "tokenizer = Qwen2Tokenizer.from_pretrained(model_path)\n",
    "tokenizer, new_token_ids, _ = add_special_tokens(tokenizer)\n",
    "\n",
    "# Image Transform Preparing\n",
    "vae_transform = ImageTransform(1024, 512, 16)\n",
    "vit_transform = ImageTransform(980, 224, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7440330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The safetensors archive passed at /checkpoint/dream_v2/transfusion/BAGEL-7B-MoT/ema.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('', 0)])\n",
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "max_mem_per_gpu = \"80GiB\"  # Modify it according to your GPU setting. On an A100, 80â€¯GiB is sufficient to load on a single GPU.\n",
    "\n",
    "device_map = infer_auto_device_map(\n",
    "    model,\n",
    "    max_memory={i: max_mem_per_gpu for i in range(torch.cuda.device_count())},\n",
    "    no_split_module_classes=[\"Bagel\", \"Qwen2MoTDecoderLayer\"],\n",
    ")\n",
    "print(device_map)\n",
    "\n",
    "same_device_modules = [\n",
    "    'language_model.model.embed_tokens',\n",
    "    'time_embedder',\n",
    "    'latent_pos_embed',\n",
    "    'vae2llm',\n",
    "    'llm2vae',\n",
    "    'connector',\n",
    "    'vit_pos_embed'\n",
    "]\n",
    "\n",
    "if torch.cuda.device_count() == 1:\n",
    "    first_device = device_map.get(same_device_modules[0], \"cuda:0\")\n",
    "    for k in same_device_modules:\n",
    "        if k in device_map:\n",
    "            device_map[k] = first_device\n",
    "        else:\n",
    "            device_map[k] = \"cuda:0\"\n",
    "else:\n",
    "    first_device = device_map.get(same_device_modules[0])\n",
    "    for k in same_device_modules:\n",
    "        if k in device_map:\n",
    "            device_map[k] = first_device\n",
    "\n",
    "# Thanks @onion-liu: https://github.com/ByteDance-Seed/Bagel/pull/8\n",
    "model = load_checkpoint_and_dispatch(\n",
    "    model,\n",
    "    checkpoint=os.path.join(model_path, \"ema.safetensors\"),\n",
    "    device_map=device_map,\n",
    "    offload_buffers=True,\n",
    "    dtype=torch.bfloat16,\n",
    "    force_hooks=True,\n",
    "    offload_folder=\"/tmp/offload\"\n",
    ")\n",
    "\n",
    "model = model.eval()\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37460163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inferencer import InterleaveInferencer\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "inferencer = InterleaveInferencer(\n",
    "    model=model, \n",
    "    vae_model=vae_model, \n",
    "    tokenizer=tokenizer, \n",
    "    vae_transform=vae_transform, \n",
    "    vit_transform=vit_transform, \n",
    "    new_token_ids=new_token_ids\n",
    ")\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbb31d9",
   "metadata": {},
   "source": [
    "## Self Reflection for T2I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37a5ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T2I inference params with thinking\n",
    "inference_hyper=dict(\n",
    "    think=True,\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    # text_temperature=0.3,\n",
    "    cfg_text_scale=4.0,\n",
    "    cfg_img_scale=1.0,\n",
    "    cfg_interval=[0.4, 1.0],\n",
    "    timestep_shift=3.0,\n",
    "    num_timesteps=50,\n",
    "    cfg_renorm_min=0.0,\n",
    "    cfg_renorm_type=\"global\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687f1353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Their original example \n",
    "prompt = 'a car made of small cars'\n",
    "print(prompt)\n",
    "print('-' * 10)\n",
    "output_dict, context = inferencer(text=prompt, return_context=True, **inference_hyper)\n",
    "display(output_dict['image'])\n",
    "print(f\"Thinking: {output_dict['text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
